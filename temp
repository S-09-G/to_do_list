"""
Configuration-driven feature engineering module.
Handles feature selection, normalization, and dimensionality reduction.
"""

import pandas as pd
import numpy as np
from sklearn.preprocessing import MinMaxScaler, StandardScaler, normalize
from sklearn.feature_extraction.text import TfidfTransformer
from sklearn.decomposition import TruncatedSVD, PCA
from typing import Tuple, List, Optional
import os
import pickle


class ConfigFeatureEngineer:
    """
    Configuration-driven feature engineering.
    """
    
    def __init__(self, config: dict, logger):
        self.config = config
        self.logger = logger
        self.preprocessing_config = config.get("preprocessing", {})
        self.selection_config = config.get("feature_selection", {})
        
        # Store fitted transformers for later use (new user assignment)
        self.fitted_transformers = {}
    
    def select_features(self, matrix: pd.DataFrame) -> pd.DataFrame:
        """
        Selects features based on configuration.
        
        Args:
            matrix: Full feature matrix
        
        Returns:
            Matrix with selected features only
        """
        
        self.logger.info("\n" + "=" * 60)
        self.logger.info("FEATURE SELECTION")
        self.logger.info("=" * 60)
        
        mode = self.selection_config.get("mode", "all")
        
        self.logger.info(f"  Mode: {mode}")
        self.logger.info(f"  Initial features: {matrix.shape[1]}")
        
        if mode == "all":
            selected_matrix = matrix.copy()
        
        elif mode == "include":
            include_list = self.selection_config.get("include_features", [])
            valid_features = [f for f in include_list if f in matrix.columns]
            selected_matrix = matrix[valid_features]
            self.logger.info(f"  Included features: {len(valid_features)}")
        
        elif mode == "exclude":
            exclude_list = self.selection_config.get("exclude_features", [])
            keep_features = [f for f in matrix.columns if f not in exclude_list]
            selected_matrix = matrix[keep_features]
            self.logger.info(f"  Excluded {len(exclude_list)} features")
        
        else:
            self.logger.warning(f"  Unknown mode '{mode}', using all features")
            selected_matrix = matrix.copy()
        
        # Apply variance threshold
        min_variance = self.selection_config.get("min_variance", 0.0)
        if min_variance > 0:
            variances = selected_matrix.var()
            high_var_features = variances[variances >= min_variance].index
            before = selected_matrix.shape[1]
            selected_matrix = selected_matrix[high_var_features]
            removed = before - selected_matrix.shape[1]
            if removed > 0:
                self.logger.info(f"  Variance filter (>= {min_variance}): removed {removed} features")
        
        self.logger.info(f"  Final features: {selected_matrix.shape[1]}")
        
        return selected_matrix
    
    def normalize(self, matrix: pd.DataFrame) -> pd.DataFrame:
        """
        Normalizes the feature matrix based on configuration.
        
        Args:
            matrix: Feature matrix
        
        Returns:
            Normalized matrix
        """
        
        method = self.preprocessing_config.get("normalization", "balanced")
        
        self.logger.info("\n" + "=" * 60)
        self.logger.info(f"NORMALIZATION (method: {method})")
        self.logger.info("=" * 60)
        
        if method == "none":
            self.logger.info("  Skipping normalization")
            return matrix
        
        elif method == "minmax":
            scaler = MinMaxScaler()
            normalized_values = scaler.fit_transform(matrix)
            self.fitted_transformers["minmax_scaler"] = scaler
            self.logger.info("  Applied Min-Max normalization (range: 0 to 1)")
            self.logger.info("  ⚠ WARNING: MinMax may cause imbalanced clusters")
        
        elif method == "standard":
            scaler = StandardScaler()
            normalized_values = scaler.fit_transform(matrix)
            self.fitted_transformers["standard_scaler"] = scaler
            self.logger.info("  Applied Standard normalization (mean=0, std=1)")
        
        elif method == "tfidf":
            tfidf = TfidfTransformer(norm='l2', use_idf=True, smooth_idf=True)
            normalized_values = tfidf.fit_transform(matrix).toarray()
            self.fitted_transformers["tfidf"] = tfidf
            self._log_tfidf_info(tfidf, matrix.columns)
        
        elif method == "log_tfidf":
            # Step 1: Log transform
            self.logger.info("  Step 1: Log transform")
            log_values = np.log1p(matrix.values)
            
            # Step 2: TF-IDF
            self.logger.info("  Step 2: TF-IDF weighting")
            tfidf = TfidfTransformer(norm='l2', use_idf=True, smooth_idf=True)
            normalized_values = tfidf.fit_transform(log_values).toarray()
            self.fitted_transformers["tfidf"] = tfidf
            self.logger.info("  Applied Log + TF-IDF normalization")
        
        elif method == "row_normalize":
            normalized_values = normalize(matrix.values, norm='l2', axis=1)
            self.logger.info("  Applied L2 row normalization")
        
        elif method == "balanced":
            normalized_values = self._balanced_normalization(matrix)
        
        else:
            self.logger.warning(f"  Unknown method '{method}', using balanced")
            normalized_values = self._balanced_normalization(matrix)
        
        normalized_matrix = pd.DataFrame(
            normalized_values,
            index=matrix.index,
            columns=matrix.columns
        )
        
        return normalized_matrix
    
    def _balanced_normalization(self, matrix: pd.DataFrame) -> np.ndarray:
        """Applies balanced multi-step normalization."""
        
        self.logger.info("  Applying BALANCED normalization (recommended):")
        
        # Step 1: Log transform
        self.logger.info("    1. Log transform to reduce skewness")
        log_values = np.log1p(matrix.values)
        
        # Step 2: TF-IDF
        self.logger.info("    2. TF-IDF to highlight distinctive preferences")
        tfidf = TfidfTransformer(norm=None, use_idf=True, smooth_idf=True)
        tfidf_values = tfidf.fit_transform(log_values).toarray()
        self.fitted_transformers["tfidf"] = tfidf
        
        # Step 3: Standard scaling
        self.logger.info("    3. Standard scaling to center data")
        scaler = StandardScaler()
        scaled_values = scaler.fit_transform(tfidf_values)
        self.fitted_transformers["standard_scaler"] = scaler
        
        # Step 4: L2 row normalize
        self.logger.info("    4. L2 row normalization for equal user weights")
        final_values = normalize(scaled_values, norm='l2', axis=1)
        
        self.logger.info("  ✓ Applied: Log → TF-IDF → Standard → L2")
        
        # Log IDF info
        idf_scores = pd.Series(tfidf.idf_, index=matrix.columns)
        self.logger.info("\n  Most distinctive features (highest IDF):")
        for feat, idf in idf_scores.sort_values(ascending=False).head(5).items():
            self.logger.info(f"    {feat}: {idf:.4f}")
        self.logger.info("  Most common features (lowest IDF):")
        for feat, idf in idf_scores.sort_values(ascending=True).head(5).items():
            self.logger.info(f"    {feat}: {idf:.4f}")
        
        return final_values
    
    def _log_tfidf_info(self, tfidf, columns):
        """Logs TF-IDF information."""
        
        self.logger.info("  Applied TF-IDF normalization")
        idf_scores = pd.Series(tfidf.idf_, index=columns)
        
        self.logger.info("  Top 5 most distinctive features (highest IDF):")
        for feat, idf in idf_scores.sort_values(ascending=False).head(5).items():
            self.logger.info(f"    {feat}: {idf:.4f}")
    
    def reduce_dimensions(self, matrix: pd.DataFrame) -> Tuple[pd.DataFrame, Optional[object]]:
        """
        Applies dimensionality reduction based on configuration.
        
        Args:
            matrix: Normalized feature matrix
        
        Returns:
            Tuple of (reduced matrix, fitted reducer model)
        """
        
        dr_config = self.preprocessing_config.get("dimensionality_reduction", {})
        
        if not dr_config.get("enabled", False):
            self.logger.info("\n  Dimensionality reduction: DISABLED")
            return matrix, None
        
        method = dr_config.get("method", "svd")
        n_components = dr_config.get("n_components", 50)
        
        # Don't reduce if we already have fewer features
        if matrix.shape[1] <= n_components:
            self.logger.info(f"\n  Dimensionality reduction: SKIPPED")
            self.logger.info(f"    Already have {matrix.shape[1]} features (target: {n_components})")
            return matrix, None
        
        self.logger.info("\n" + "=" * 60)
        self.logger.info(f"DIMENSIONALITY REDUCTION (method: {method})")
        self.logger.info("=" * 60)
        self.logger.info(f"  Original dimensions: {matrix.shape[1]}")
        self.logger.info(f"  Target dimensions: {n_components}")
        
        random_state = self.config.get("clustering", {}).get("random_state", 42)
        
        if method == "svd":
            reducer = TruncatedSVD(
                n_components=n_components,
                random_state=random_state,
                n_iter=10
            )
        elif method == "pca":
            reducer = PCA(
                n_components=n_components,
                random_state=random_state
            )
        else:
            self.logger.warning(f"  Unknown method '{method}', using SVD")
            reducer = TruncatedSVD(n_components=n_components, random_state=random_state)
        
        reduced_values = reducer.fit_transform(matrix.values)
        self.fitted_transformers["dim_reducer"] = reducer
        
        # Create column names
        columns = [f"component_{i+1}" for i in range(n_components)]
        
        reduced_matrix = pd.DataFrame(
            reduced_values,
            index=matrix.index,
            columns=columns
        )
        
        # Log explained variance
        explained_var = reducer.explained_variance_ratio_.sum() * 100
        self.logger.info(f"  Explained variance: {explained_var:.2f}%")
        
        self.logger.info("  Top 5 components:")
        for i in range(min(5, n_components)):
            var = reducer.explained_variance_ratio_[i] * 100
            self.logger.info(f"    Component {i+1}: {var:.2f}%")
        
        return reduced_matrix, reducer
    
    def save_matrix(self, matrix: pd.DataFrame, filename: str) -> str:
        """Saves matrix to processed folder."""
        
        output_dir = self.config["paths"]["processed_data"]
        os.makedirs(output_dir, exist_ok=True)
        
        output_path = os.path.join(output_dir, filename)
        matrix.to_csv(output_path)
        
        self.logger.info(f"  Saved: {output_path}")
        
        return output_path
    
    def save_transformers(self) -> str:
        """Saves fitted transformers for later use."""
        
        output_dir = self.config["paths"]["models"]
        os.makedirs(output_dir, exist_ok=True)
        
        output_path = os.path.join(output_dir, "feature_transformers.pkl")
        
        with open(output_path, "wb") as f:
            pickle.dump(self.fitted_transformers, f)
        
        self.logger.info(f"  Transformers saved: {output_path}")
        
        return output_path