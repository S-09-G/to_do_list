"""
Clustering algorithms for user segmentation.
"""

import pandas as pd
import numpy as np
import os
import pickle
from sklearn.cluster import MiniBatchKMeans
from sklearn.metrics import silhouette_score


def find_optimal_clusters(
    matrix: pd.DataFrame,
    config: dict,
    logger,
    k_range: tuple = (3, 15)
) -> dict:
    """
    Finds optimal number of clusters using Elbow Method and Silhouette Score.
    
    Args:
        matrix: User-genre feature matrix
        config: Configuration dictionary
        logger: Logger instance
        k_range: Tuple of (min_k, max_k) to test
    
    Returns:
        Dictionary with evaluation metrics for each k
    """
    
    logger.info("=" * 50)
    logger.info("FINDING OPTIMAL NUMBER OF CLUSTERS")
    logger.info("=" * 50)
    
    random_state = config["clustering"]["random_state"]
    batch_size = config["clustering"]["batch_size"]
    
    k_min, k_max = k_range
    logger.info(f"Testing k from {k_min} to {k_max}...")
    
    results = {
        "k_values": [],
        "inertia": [],          # For Elbow Method
        "silhouette": []        # For Silhouette Score
    }
    
    matrix_values = matrix.values
    
    for k in range(k_min, k_max + 1):
        logger.info(f"  Testing k={k}...")
        
        # Fit MiniBatch KMeans
        kmeans = MiniBatchKMeans(
            n_clusters=k,
            random_state=random_state,
            batch_size=batch_size,
            n_init=10
        )
        
        cluster_labels = kmeans.fit_predict(matrix_values)
        
        # Calculate metrics
        inertia = kmeans.inertia_
        silhouette = silhouette_score(matrix_values, cluster_labels)
        
        results["k_values"].append(k)
        results["inertia"].append(inertia)
        results["silhouette"].append(silhouette)
        
        logger.info(f"    Inertia: {inertia:,.2f} | Silhouette: {silhouette:.4f}")
    
    # Find best k based on silhouette score
    best_idx = np.argmax(results["silhouette"])
    best_k = results["k_values"][best_idx]
    best_silhouette = results["silhouette"][best_idx]
    
    logger.info("-" * 50)
    logger.info(f"Best k based on Silhouette Score: {best_k} (score: {best_silhouette:.4f})")
    
    return results


def train_clustering_model(
    matrix: pd.DataFrame,
    config: dict,
    logger,
    n_clusters: int = None
) -> tuple:
    """
    Trains the final clustering model.
    
    Args:
        matrix: User-genre feature matrix
        config: Configuration dictionary
        logger: Logger instance
        n_clusters: Number of clusters (uses config value if None)
    
    Returns:
        Tuple of (trained model, cluster labels)
    """
    
    if n_clusters is None:
        n_clusters = config["clustering"]["n_clusters"]
    
    random_state = config["clustering"]["random_state"]
    batch_size = config["clustering"]["batch_size"]
    
    logger.info("=" * 50)
    logger.info("TRAINING CLUSTERING MODEL")
    logger.info("=" * 50)
    logger.info(f"Algorithm: MiniBatch K-Means")
    logger.info(f"Number of clusters: {n_clusters}")
    
    # Train model
    model = MiniBatchKMeans(
        n_clusters=n_clusters,
        random_state=random_state,
        batch_size=batch_size,
        n_init=10
    )
    
    matrix_values = matrix.values
    cluster_labels = model.fit_predict(matrix_values)
    
    # Calculate final metrics
    inertia = model.inertia_
    silhouette = silhouette_score(matrix_values, cluster_labels)
    
    logger.info(f"Training complete!")
    logger.info(f"  Inertia: {inertia:,.2f}")
    logger.info(f"  Silhouette Score: {silhouette:.4f}")
    
    # Cluster distribution
    logger.info("-" * 50)
    logger.info("CLUSTER DISTRIBUTION:")
    unique, counts = np.unique(cluster_labels, return_counts=True)
    for cluster_id, count in zip(unique, counts):
        pct = (count / len(cluster_labels)) * 100
        logger.info(f"  Cluster {cluster_id}: {count:,} users ({pct:.1f}%)")
    
    return model, cluster_labels


def save_model(model, config: dict, logger) -> str:
    """
    Saves the trained clustering model to disk.
    
    Args:
        model: Trained clustering model
        config: Configuration dictionary
        logger: Logger instance
    
    Returns:
        Path where model was saved
    """
    
    output_dir = config["paths"]["models"]
    os.makedirs(output_dir, exist_ok=True)
    
    model_path = os.path.join(output_dir, "clustering_model.pkl")
    
    with open(model_path, "wb") as f:
        pickle.dump(model, f)
    
    logger.info(f"Model saved to: {model_path}")
    
    return model_path


def save_cluster_assignments(
    matrix: pd.DataFrame,
    cluster_labels: np.ndarray,
    config: dict,
    logger
) -> str:
    """
    Saves user cluster assignments to a CSV file.
    
    Args:
        matrix: User-genre matrix (to get user IDs from index)
        cluster_labels: Array of cluster assignments
        config: Configuration dictionary
        logger: Logger instance
    
    Returns:
        Path where assignments were saved
    """
    
    output_dir = config["paths"]["processed_data"]
    os.makedirs(output_dir, exist_ok=True)
    
    # Create assignments dataframe
    assignments = pd.DataFrame({
        "user_id": matrix.index,
        "cluster": cluster_labels
    })
    
    output_path = os.path.join(output_dir, "cluster_assignments.csv")
    assignments.to_csv(output_path, index=False)
    
    logger.info(f"Cluster assignments saved to: {output_path}")
    
    return output_path











"""
Step 4: Cluster users based on genre preferences.
Run from project root: python run_clustering.py
"""

import pandas as pd
from src.config_loader import load_config
from src.logger_setup import setup_logger
from src.models.clustering import (
    find_optimal_clusters,
    train_clustering_model,
    save_model,
    save_cluster_assignments
)


def main():
    # Load config
    config = load_config()
    
    # Setup logger
    logger = setup_logger(
        log_dir=config["paths"]["logs"],
        log_filename="clustering.log",
        level=config["logging"]["level"]
    )
    
    logger.info("Starting clustering pipeline...")
    
    # Load feature matrix
    matrix_path = config["paths"]["processed_data"] + "user_genre_matrix.csv"
    logger.info(f"Loading feature matrix from: {matrix_path}")
    matrix = pd.read_csv(matrix_path, index_col=0)
    logger.info(f"Loaded matrix: {matrix.shape[0]:,} users Ã— {matrix.shape[1]} genres")
    
    # Step 1: Find optimal number of clusters
    results = find_optimal_clusters(matrix, config, logger, k_range=(3, 15))
    
    # Step 2: Ask user for final k (or use best from silhouette)
    best_k = results["k_values"][results["silhouette"].index(max(results["silhouette"]))]
    logger.info(f"Recommended k: {best_k}")
    
    # Step 3: Train final model with best k
    model, cluster_labels = train_clustering_model(matrix, config, logger, n_clusters=best_k)
    
    # Step 4: Save model and assignments
    save_model(model, config, logger)
    save_cluster_assignments(matrix, cluster_labels, config, logger)
    
    logger.info("Clustering complete!")
    
    return model, cluster_labels, results


if __name__ == "__main__":
    model, labels, results = main()