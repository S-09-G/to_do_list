"""
CONFIGURATION-DRIVEN CLUSTERING PIPELINE

Usage:
    python run_pipeline.py                    # Run full pipeline with menu
    python run_pipeline.py --step load        # Only load data
    python run_pipeline.py --step features    # Only feature engineering
    python run_pipeline.py --step clustering  # Only clustering
    python run_pipeline.py --validate         # Validate data only
"""

import argparse
import pandas as pd
import os
from src.config_loader import load_config
from src.logger_setup import setup_logger
from src.data_processing.config_data_loader import ConfigDataLoader, PrimaryDataValidator
from src.features.config_feature_engineer import ConfigFeatureEngineer
from src.models.config_clustering import ConfigClustering


def run_data_pipeline(config: dict, logger) -> pd.DataFrame:
    """Phase 1: Load and validate data."""
    
    logger.info("\n" + "#" * 70)
    logger.info("# PHASE 1: DATA LOADING")
    logger.info("#" * 70)
    
    # Load data
    loader = ConfigDataLoader(config, logger)
    primary_df = loader.load_primary()
    
    # Validate
    validator = PrimaryDataValidator(config, logger)
    report = validator.validate(primary_df)
    
    if not report["valid"]:
        logger.error("Data validation failed! Please fix issues.")
        raise ValueError("Data validation failed")
    
    # Load secondary (if any enabled)
    loader.load_secondary()
    merged_df = loader.merge_all()
    
    # Get feature matrix
    feature_matrix, feature_cols = loader.get_feature_matrix()
    
    # Save
    if config.get("output", {}).get("save_intermediate", True):
        output_dir = config["paths"]["processed_data"]
        os.makedirs(output_dir, exist_ok=True)
        
        feature_path = os.path.join(output_dir, "feature_matrix_raw.csv")
        feature_matrix.to_csv(feature_path)
        logger.info(f"\nFeature matrix saved: {feature_path}")
    
    return feature_matrix


def run_feature_pipeline(config: dict, logger, matrix: pd.DataFrame = None) -> pd.DataFrame:
    """Phase 2: Feature engineering."""
    
    logger.info("\n" + "#" * 70)
    logger.info("# PHASE 2: FEATURE ENGINEERING")
    logger.info("#" * 70)
    
    # Load if not provided
    if matrix is None:
        matrix_path = os.path.join(config["paths"]["processed_data"], "feature_matrix_raw.csv")
        logger.info(f"Loading from: {matrix_path}")
        matrix = pd.read_csv(matrix_path, index_col=0)
    
    logger.info(f"Input: {matrix.shape[0]:,} users × {matrix.shape[1]} features")
    
    # Feature engineering
    engineer = ConfigFeatureEngineer(config, logger)
    
    # Select features
    matrix = engineer.select_features(matrix)
    
    # Normalize
    normalized_matrix = engineer.normalize(matrix)
    
    # Save normalized
    if config.get("output", {}).get("save_intermediate", True):
        engineer.save_matrix(normalized_matrix, "feature_matrix_normalized.csv")
    
    # Dimensionality reduction
    final_matrix, reducer = engineer.reduce_dimensions(normalized_matrix)
    
    # Save final
    engineer.save_matrix(final_matrix, "feature_matrix_final.csv")
    
    # Save transformers
    if config.get("output", {}).get("save_models", True):
        engineer.save_transformers()
    
    return final_matrix


def run_clustering_pipeline(config: dict, logger, matrix: pd.DataFrame = None):
    """Phase 3: Clustering with interactive menu."""
    
    logger.info("\n" + "#" * 70)
    logger.info("# PHASE 3: CLUSTERING")
    logger.info("#" * 70)
    
    # Load if not provided
    if matrix is None:
        matrix_path = os.path.join(config["paths"]["processed_data"], "feature_matrix_final.csv")
        logger.info(f"Loading from: {matrix_path}")
        matrix = pd.read_csv(matrix_path, index_col=0)
    
    logger.info(f"Matrix: {matrix.shape[0]:,} users × {matrix.shape[1]} features")
    
    # Initialize clustering
    clusterer = ConfigClustering(config, logger)
    
    # Interactive menu
    print("\n" + "=" * 60)
    print("CLUSTERING OPTIONS")
    print("=" * 60)
    print("\n1. Compare all enabled algorithms")
    print("2. Run single algorithm (find best k)")
    print("3. Train final model (specify algorithm and k)")
    print("4. Exit")
    
    enabled = clusterer.get_enabled_algorithms()
    print(f"\nEnabled algorithms: {[a['display_name'] for a in enabled]}")
    print("=" * 60)
    
    choice = input("\nEnter choice (1/2/3/4): ").strip()
    
    if choice == "4":
        return None
    
    elif choice == "1":
        # Compare all
        all_results = clusterer.compare_algorithms(matrix)
        clusterer.save_comparison_report(all_results)
        
        # Offer to train
        train = input("\nTrain a final model? (y/n): ").strip().lower()
        if train == 'y':
            _interactive_train(clusterer, matrix, all_results, enabled)
        
        return all_results
    
    elif choice == "2":
        # Single algorithm
        algo_config = _select_algorithm(enabled)
        results = clusterer.find_optimal_k(matrix, algo_config)
        
        train = input("\nTrain final model? (y/n): ").strip().lower()
        if train == 'y':
            suggested_k = results.get("best_k", 10)
            k = input(f"Enter k (suggested={suggested_k}): ").strip()
            k = int(k) if k else suggested_k
            
            model, labels, probs = clusterer.train_model(matrix, algo_config["name"], k)
            clusterer.save_model(model, algo_config["name"])
            clusterer.save_assignments(matrix, labels, algo_config["name"], probs)
        
        return results
    
    elif choice == "3":
        # Direct train
        algo_config = _select_algorithm(enabled)
        k = int(input("Enter number of clusters (k): ").strip())
        
        model, labels, probs = clusterer.train_model(matrix, algo_config["name"], k)
        clusterer.save_model(model, algo_config["name"])
        clusterer.save_assignments(matrix, labels, algo_config["name"], probs)
        
        return {"model": model, "labels": labels}
    
    return None


def _select_algorithm(enabled: list) -> dict:
    """Helper to select algorithm."""
    
    print("\nSelect algorithm:")
    for i, algo in enumerate(enabled, 1):
        print(f"  {i}. {algo['display_name']}")
    
    idx = int(input("Enter number: ").strip()) - 1
    return enabled[idx]


def _interactive_train(clusterer, matrix, all_results, enabled):
    """Helper for training after comparison."""
    
    print("\nSelect algorithm:")
    for i, algo in enumerate(enabled, 1):
        best_k = all_results.get(algo["name"], {}).get("best_k", "N/A")
        print(f"  {i}. {algo['display_name']} (best k={best_k})")
    
    idx = int(input("Enter number: ").strip()) - 1
    algo_config = enabled[idx]
    algo_name = algo_config["name"]
    
    suggested_k = all_results.get(algo_name, {}).get("best_k", 10)
    k = input(f"Enter k (suggested={suggested_k}): ").strip()
    k = int(k) if k else suggested_k
    
    model, labels, probs = clusterer.train_model(matrix, algo_name, k)
    clusterer.save_model(model, algo_name)
    clusterer.save_assignments(matrix, labels, algo_name, probs)


def main():
    """Main entry point."""
    
    parser = argparse.ArgumentParser(description="Configuration-driven clustering pipeline")
    parser.add_argument("--step", choices=["load", "features", "clustering", "all"], default="all")
    parser.add_argument("--validate", action="store_true", help="Only validate data")
    parser.add_argument("--config", default="configs/config.yaml")
    
    args = parser.parse_args()
    
    # Load config
    config = load_config(args.config)
    
    # Setup logger
    logger = setup_logger(
        log_dir=config["paths"]["logs"],
        log_filename=config["logging"]["log_filename"],
        level=config["logging"]["level"]
    )
    
    logger.info("=" * 70)
    logger.info("CONFIGURATION-DRIVEN CLUSTERING PIPELINE")
    logger.info("=" * 70)
    
    if args.validate:
        # Only validate
        loader = ConfigDataLoader(config, logger)
        df = loader.load_primary()
        validator = PrimaryDataValidator(config, logger)
        validator.validate(df)
        return
    
    # Run pipeline
    if args.step == "load":
        run_data_pipeline(config, logger)
    
    elif args.step == "features":
        run_feature_pipeline(config, logger)
    
    elif args.step == "clustering":
        run_clustering_pipeline(config, logger)
    
    else:  # all
        matrix = run_data_pipeline(config, logger)
        matrix = run_feature_pipeline(config, logger, matrix)
        run_clustering_pipeline(config, logger, matrix)
    
    logger.info("\n" + "=" * 70)
    logger.info("PIPELINE COMPLETE")
    logger.info("=" * 70)


if __name__ == "__main__":
    main()